{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: AdaBoost (Adaptive Boosting)\n",
    "#### **Objective**\n",
    "In this task, you will implement the **AdaBoost** algorithm for binary classification. AdaBoost is an ensemble method that combines multiple \"weak learners\" (typically simple decision trees) to create a strong classifier.\n",
    "\n",
    "#### **Overview**\n",
    "The AdaBoost algorithm works by iteratively training weak learners on the dataset. In each iteration, it adjusts the weights of the training samples so that the next learner focuses more on the samples that were misclassified by the previous ones.\n",
    "\n",
    "The algorithm proceeds as follows (for $T$ rounds):\n",
    "1. **Initialize weights**: Assign equal weights $w_i = 1/n$ to all $n$ training samples.\n",
    "2. **Iterate $t=1$ to $T$**:\n",
    "   - **(a) Fit a weak learner**: Train a base classifier $h_t$ using the weighted samples.\n",
    "   - **(b) Compute weighted error**: Calculate the error $\\varepsilon_t$ as the sum of weights of misclassified samples.\n",
    "   - **(c) Compute learner weight**: Calculate $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$.\n",
    "   - **(d) Update sample weights**: Increase weights for misclassified samples: $w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_t y_i h_t(x_i))$.\n",
    "   - **(e) Normalize weights**: Scale $w_i$ so that $\\sum w_i = 1$.\n",
    "3. **Final Prediction**: The final model aggregates predictions: $F_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$, and the predicted class is $\\text{sign}(F_T(x))$.\n",
    "\n",
    "---\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Compute Weighted Error**\n",
    "- **Function to complete**: `_compute_error(self, y, y_pred, w)`\n",
    "- **Purpose**: Calculate the total weight of misclassified samples.\n",
    "- **Formula**: $\\varepsilon_t = \\sum_{i=1}^n w_i \\cdot \\mathbb{1}(h_t(x_i) \\neq y_i)$\n",
    "\n",
    "##### **SubTask 2: Compute Learner Weight**\n",
    "- **Function to complete**: `_compute_alpha(self, error)`\n",
    "- **Purpose**: Determine the importance ($\\alpha_t$) of the current weak learner based on its error.\n",
    "- **Formula**: $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$\n",
    "\n",
    "##### **SubTask 3: Update Sample Weights**\n",
    "- **Function to complete**: `_update_weights(self, w, alpha, y, y_pred)`\n",
    "- **Purpose**: Update weights to penalize misclassifications and normalize them.\n",
    "- **Formula**: $w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_t y_i h_t(x_i))$, followed by normalization.\n",
    "\n",
    "##### **SubTask 4: Train AdaBoost**\n",
    "- **Function to complete**: `fit(self, X, y)`\n",
    "- **Purpose**: Implement the main loop of the AdaBoost algorithm.\n",
    "- **Steps**:\n",
    "  1. Initialize weights $w$.\n",
    "  2. Loop $T$ times:\n",
    "     - Train weak learner.\n",
    "     - Compute error and alpha.\n",
    "     - Update weights.\n",
    "     - Store learner and alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T04:32:30.740648700Z",
     "start_time": "2026-02-12T04:32:30.267937400Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoost:\n",
    "    \"\"\"\n",
    "    AdaBoost (Adaptive Boosting) classifier for binary classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=50, seed=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.seed = seed\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        \n",
    "    def _compute_error(self, y, y_pred, w):\n",
    "        \"\"\"\n",
    "        Calculates the weighted training error.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function\n",
    "        # Calculate the weighted sum of incorrect predictions\n",
    "        error = np.sum(w *(y_pred != y))\n",
    "        return error\n",
    "    \n",
    "    def _compute_alpha(self, error):\n",
    "        \"\"\"\n",
    "        Computes the weight (alpha) of the current weak learner.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10  # Tiny constant to avoid division by zero\n",
    "        # TODO: Implement this function\n",
    "        # Calculate alpha based on the error\n",
    "        eps = max(epsilon, error)\n",
    "        alpha = 0.5 * np.log((1 - eps) / (eps))\n",
    "        return alpha\n",
    "    \n",
    "    def _update_weights(self, w, alpha, y, y_pred):\n",
    "        \"\"\"\n",
    "        Updates and normalizes sample weights.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function\n",
    "        # Step 1: Update weights (increase weight for misclassified samples)\n",
    "        # Hint: y * y_pred is 1 if correct, -1 if incorrect\n",
    "        w = w * np.exp(-alpha * y * y_pred)\n",
    "        \n",
    "        # Step 2: Normalize weights so they sum to 1\n",
    "        w = w / np.sum(w)\n",
    "        \n",
    "        return w\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the AdaBoost model.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Clear previous models\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "        \n",
    "        # IMPORTANT: Convert labels to {-1, 1} if they are {0, 1}\n",
    "        if set(np.unique(y)) == {0, 1}:\n",
    "            y = np.where(y == 0, -1, 1)\n",
    "            \n",
    "        # Initialize weights uniformly: w_i = 1/n\n",
    "        w = np.full(n_samples, 1.0 / n_samples)\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "        for t in range(self.n_estimators):\n",
    "            # a) Fit a weak learner on weighted data\n",
    "            stump = DecisionTreeClassifier(max_depth=1, random_state=self.seed)\n",
    "            stump.fit(X, y, sample_weight=w)\n",
    "            \n",
    "            # Predict on training data to compute error\n",
    "            y_pred = stump.predict(X)\n",
    "            \n",
    "            # TODO: Implement the boosting steps\n",
    "            # b) call _compute_error\n",
    "            error = self._compute_error(y, y_pred, w)\n",
    "            \n",
    "            # c) call _compute_alpha\n",
    "            alpha = self._compute_alpha(error)\n",
    "            \n",
    "            # d) call _update_weights\n",
    "            w = self._update_weights(w, alpha, y, y_pred)\n",
    "            \n",
    "            # Store the trained model and its weight\n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for X.\n",
    "        \"\"\"\n",
    "        # Collect predictions from all weak learners\n",
    "        weak_preds = np.array([stump.predict(X) for stump in self.models])\n",
    "        \n",
    "        # Weighted sum of predictions\n",
    "        weighted_sum = np.dot(self.alphas, weak_preds)\n",
    "        \n",
    "        # Return sign of the sum\n",
    "        final_preds = np.sign(weighted_sum)\n",
    "        \n",
    "        # Map 0s to 1s (rare edge case where sum is exactly 0)\n",
    "        final_preds[final_preds == 0] = 1\n",
    "        \n",
    "        return final_preds"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "Let's test your implementation on the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T04:32:31.001352900Z",
     "start_time": "2026-02-12T04:32:30.756452700Z"
    }
   },
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Convert y from {0, 1} to {-1, 1}\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = AdaBoost(n_estimators=50, seed=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9649\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
