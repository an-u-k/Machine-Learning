{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Random Forest\n",
    "#### **Objective**\n",
    "In this lab, you will implement missing parts of a `RandomForest` classifier. This classifier is an ensemble of decision trees that are trained using bootstrap sampling and random subspace method for feature selection. (**Note:** This is not a *trditional* Random Forest, since we are using the random subspace method — each *tree* sees a subsample of features — instead of the random attribute method, where each *node* sees a subsample of features.)\n",
    "\n",
    "#### **Overview**\n",
    "A `RandomForest` model is built using the following steps:\n",
    "1. **Bootstrap Sampling**: Randomly selecting samples with replacement to train each tree.\n",
    "2. **Random Feature Selection**: Choosing a subset of features for training each tree.\n",
    "3. **Training Decision Trees**: Fitting multiple decision trees using the sampled dataset.\n",
    "4. **Aggregating Predictions**: Using majority voting to determine the final prediction.\n",
    "\n",
    "Your task is to implement the missing parts (marked as `TODO`) in the given code.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement Bootstrap Sampling**\n",
    "- **Function to complete**: `_bootstrap_sample(self, X, y)`\n",
    "- **Purpose**: Generate a bootstrap sample by randomly selecting data points **with replacement**.\n",
    "- **Implementation details**:\n",
    "  - Randomly sample `len(X)` instances from `X` and `y`, allowing repetition.\n",
    "  - Return:\n",
    "    - `X_sample`: The sampled feature matrix.\n",
    "    - `y_sample`: The corresponding labels.\n",
    "    - `indices`: The indices of the selected samples.\n",
    "- **Steps**:\n",
    "  1. Use `np.random.choice()` to sample indices from `X`.\n",
    "  2. Select the corresponding rows from `X` and `y` using the sampled indices.\n",
    "  3. Return `(X_sample, y_sample, indices)`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `replace=True` in `np.random.choice()` to allow repeated selections.\n",
    "- The number of selected indices should be equal to the number of rows in `X`.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement Random Feature Selection**\n",
    "- **Function to complete**: `_select_random_features(self, X, n_features)`\n",
    "- **Purpose**: Select a subset of features randomly to be used for training each tree.\n",
    "- **Implementation details**:\n",
    "  - Determine the number of features to select based on `self.max_features`:\n",
    "    - `\"sqrt\"` → `sqrt(n_features)`\n",
    "    - `\"log2\"` → `log2(n_features)`\n",
    "    - `int` value → Use the given number of features.\n",
    "  - Randomly select `max_features` feature indices.\n",
    "  - Return:\n",
    "    - `X_subset`: The feature matrix containing only the selected features.\n",
    "    - `feature_indices`: The indices of the selected features.\n",
    "- **Steps**:\n",
    "  1. Compute the number of features to select (`max_features`).\n",
    "  2. Randomly choose `max_features` feature indices using `np.random.choice()`.\n",
    "  3. Extract the corresponding columns from `X` using these indices.\n",
    "  4. Return `(X_subset, feature_indices)`.\n",
    "\n",
    "**Hints:**\n",
    "- Ensure `max_features` does not exceed `n_features`.\n",
    "- Use `replace=False` in `np.random.choice()` to prevent selecting the same feature multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 3: Train Individual Decision Trees**\n",
    "- **Function to complete**: `fit(self, X, y)`\n",
    "- **Purpose**: Train `n_estimators` decision trees using bootstrap samples and random feature selection.\n",
    "- **Implementation details**:\n",
    "  - **For each tree:**\n",
    "    1. **Create a bootstrap sample** from `X` and `y` using `_bootstrap_sample()`.\n",
    "    2. **Select a subset of features** from the bootstrap sample using `_select_random_features()`.\n",
    "    3. **Train a `DecisionTreeClassifier`** using the subset of features.\n",
    "    4. Store the trained tree and selected feature indices.\n",
    "\n",
    "**Steps:**\n",
    "1. Convert `X` and `y` into NumPy arrays if they are Pandas DataFrames/Series.\n",
    "2. Initialize an empty list `self.trees` to store trained trees.\n",
    "3. For each tree (`self.n_estimators`):\n",
    "   - Call `_bootstrap_sample()` to generate a training sample.\n",
    "   - Call `_select_random_features()` to select features for training.\n",
    "   - Train a `DecisionTreeClassifier` on the sampled data.\n",
    "   - Store the trained tree and the selected feature indices in `self.trees`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `np.random.seed(self.seed)` for reproducibility."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T04:26:50.287761800Z",
     "start_time": "2026-02-12T04:26:50.245270700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A basic implementation of a Random Forest algorithm for classification.\n",
    "\n",
    "    This class builds an ensemble of decision trees using bootstrap sampling and random feature selection.\n",
    "    It combines predictions from individual trees using majority voting to produce a robust classification model.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Number of decision trees in the forest.\n",
    "    max_depth : int or None\n",
    "        Maximum depth of each tree. If None, the trees grow until pure leaves or until reaching `min_samples_split`.\n",
    "    min_samples_split : int\n",
    "        Minimum number of samples required to split an internal node in a tree.\n",
    "    max_features : str or int\n",
    "        Number of features to consider when looking for the best split.\n",
    "        - \"sqrt\": Square root of the total number of features.\n",
    "        - \"log2\": Base-2 logarithm of the total number of features.\n",
    "        - int: A specific number of features to use.\n",
    "    seed : int or None\n",
    "        Random seed for reproducibility. If None, results will vary between runs.\n",
    "    trees : list\n",
    "        A list containing tuples of trained decision trees and their selected feature indices/names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, max_features=\"sqrt\", seed=None):\n",
    "        \"\"\"\n",
    "        Initializes the Random Forest classifier.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_estimators : int, default=10\n",
    "            Number of decision trees to build in the forest.\n",
    "        max_depth : int or None, default=None\n",
    "            Maximum depth of each decision tree. If None, trees grow until they are pure or cannot be split further.\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum number of samples required to split an internal node.\n",
    "        max_features : str or int, default=\"sqrt\"\n",
    "            Number of features to consider when splitting nodes:\n",
    "            - \"sqrt\": Square root of the total number of features.\n",
    "            - \"log2\": Base-2 logarithm of the total number of features.\n",
    "            - int: Specific number of features.\n",
    "        seed : int or None, default=None\n",
    "            Random seed for reproducibility. If None, results may vary across runs.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.seed =  seed\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates a bootstrap sample of the input dataset by sampling with replacement.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        y : numpy.ndarray\n",
    "            Target vector with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple (X_sample, y_sample, indices) where:\n",
    "            - X_sample is the feature matrix of the bootstrap sample.\n",
    "            - y_sample is the target vector of the bootstrap sample.\n",
    "            - indices are the indices of the selected samples.\n",
    "        \"\"\"\n",
    "        # TODO implement this function\n",
    "        # Step 1: Use np.random.choice() to sample indices from X.\n",
    "        indices = np.random.choice(X.shape[0],\n",
    "                                   X.shape[0],\n",
    "                                   replace=True)\n",
    "\n",
    "        # Step 2: Select the corresponding rows from X and y using the sampled indices.\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y[indices]\n",
    "\n",
    "        return X_sample, y_sample, indices\n",
    "\n",
    "    def _select_random_features(self, X, n_features):\n",
    "        \"\"\"\n",
    "        Selects a random subset of features to use for a decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        n_features : int\n",
    "            Total number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple (X_subset, feature_indices) where:\n",
    "            - X_subset is the feature matrix containing only the selected features.\n",
    "            - feature_indices are the indices of the selected features.\n",
    "        \"\"\"\n",
    "        # TODO implement this function\n",
    "        # Step 1: Compute the number of features to select (max_features).\n",
    "        if self.max_features == \"sqrt\":\n",
    "            max_features = np.sqrt(n_features)\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = np.log2(n_features)\n",
    "        else:\n",
    "            max_features = 5;\n",
    "\n",
    "        # Step 2: Randomly choose max_features feature indices using np.random.choice().\n",
    "        feature_indices = np.random.choice(n_features, size=int(max_features), replace=False)\n",
    "\n",
    "        # Step 3: Extract the corresponding columns from X using these indices.\n",
    "        X_subset = X[:,feature_indices]\n",
    "        return X_subset, feature_indices\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the Random Forest model by creating an ensemble of decision trees.\n",
    "\n",
    "        Each tree is trained on a bootstrap sample of the data and a random subset of features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        y : numpy.ndarray\n",
    "            Target vector with shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.feature_names = None\n",
    "\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # TODO implement this function\n",
    "            # Create bootstrap sample\n",
    "            # Step 1: Call _bootstrap_sample() to generate a training sample.\n",
    "            X_sample, y_sample, _ = self._bootstrap_sample(X, y)\n",
    "            # Step 2: Call _select_random_features() to select features for training.\n",
    "            X_subset, feature_indices = self._select_random_features(X_sample,n_features)\n",
    "\n",
    "            # Step 3: Train a DecisionTreeClassifier on the sampled data.\n",
    "            tree = DecisionTreeClassifier(\n",
    "                        max_depth=self.max_depth,\n",
    "                        min_samples_split=self.min_samples_split,\n",
    "                        random_state=self.seed\n",
    "                        )\n",
    "            tree.fit(X_subset, y_sample)\n",
    "\n",
    "            # Step 4: Store the trained tree and the selected feature indices in self.trees.\n",
    "            selected_features = feature_indices\n",
    "            self.trees.append((tree, selected_features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for the input data using the trained Random Forest.\n",
    "\n",
    "        Predictions are made by aggregating votes from all individual trees (majority voting).\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Predicted class labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))\n",
    "        for i, (tree, selected_features) in enumerate(self.trees):\n",
    "            feature_indices = selected_features\n",
    "\n",
    "            predictions[:, i] = tree.predict(X[:, feature_indices])\n",
    "\n",
    "        return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=1, arr=predictions)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T04:26:37.358775400Z",
     "start_time": "2026-02-12T04:26:36.949603800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into a training and a test set\n",
    "X_mix, X_val, y_mix, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mix, y_mix, test_size=0.25, random_state=0, stratify=y_mix)\n",
    "rf = RandomForest(n_estimators=5, max_depth=5, max_features=\"sqrt\", seed=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
